---
license: apache-2.0
tags:
  - kernel
---

# Flash-Attn2-NPU

This project is a replacement implementation for: [transformers/integrations/npu_flash_attention.py](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/npu_flash_attention.py)
